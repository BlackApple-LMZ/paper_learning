### 摘要：

我们在ImageNet大规模视觉识别挑战赛2014（ILSVRC14）上提出了一种代号为Inception的深度卷积神经网络结构，并在分类和检测上取得了新的最好结果。该体系结构的主要特点是提高了网络内计算资源的利用率。这是通过精心设计实现的，该设计允许在保持计算预算不变的同时增加网络的深度和宽度。为了优化质量（quality），架构决策基于Hebbian原则和多尺度处理。在我们为ilsvrc 14提交的文件中使用的一种特殊形式称为googlenet，它是一个22层深的网络，其质量将在分类和检测的范围内进行评估。

就是说准确率提升的同时，还得保持运算量不提升；

#### 动机和高层考虑：
改善深层神经网络性能最直接的方法是增加它们的大小。这包括增加网络的深度(层数)及其宽度：每层的单元数。

这样做有两个缺点：一是容易过拟合，二是计算资源的急剧增加；

#### 网络结构细节：
讲了很多的设计理念，没有太看懂，第一个想法就是网络自己设计选用3\*3还是5\*5还是maxpooling的网络，这样能增加网络的宽度与深度；

#### Inception结构：
![image](https://github.com/BlackApple-LMZ/paper_learning/edit/main/2021/0205-inceptionv1%20Going%20deeper%20with%20convolutions/11.png)

左边的朴素结构是没有用1\*1卷积优化的结构，计算复杂，尤其是池化层不改变输入层的维度就直接到了下一层，所以层数堆叠会越来越多；右面是inception网络的第二个想法：先进行1\*1卷积减少通道数，然后再用3\*3或者5\*5的卷积核降维，可以减小计算量。同时又用了一次非线性。

Inception网络就是由上述类型的模块相互叠加而成的网络，由于技术原因(训练期间的内存效率)，似乎只在较高层开始使用初始模块，而以传统的卷积方式保持较低层的使用是有益的。这个模型的好处是可以增加单元数量，就是更宽一些，然后通过先使用1\*1卷积可以降低计算复杂性；另一个实际有用的方面是，它与直觉保持一致，即视觉信息应该在不同的尺度上进行处理，然后进行聚合，以便下一阶段能够同时从不同的尺度中提取特征。

文章认为解决上述两个缺点的根本方法是将全连接甚至一般的卷积都转化为稀疏连接（作者认为卷积是一种稀疏结构，然后inception把相关性较强的特征聚到一起，也是一种稀疏性的体现）。所以，现在的问题是有没有一种方法，既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能。大量的文献表明可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，据此论文提出了名为Inception 的结构来实现此目的。就是将3*3，5*5的矩阵都聚集到一起。Inception 结构的主要思路是怎样用密集成分来近似最优的局部稀疏结构。

### 主要突破：
- 1 在网络结构上与之前的网络结构有比较大的差异，而且深度也达到了22层。在 GoogLeNet 上开始出现了分支，而不是一条线连到底，这是最直观的差异，也被称作 Inception module，如下图所示。从图中可以看到， 每个 module 中采用了不同 size 的 kernel，然后在将特征图叠加，实际上起到了一个图像金字塔的作用，即 所谓的 multiple resolution。
- 2 上图中有很多 1x1 的卷积核，这里的1x1的卷积操作与之前讲到是不一样的，这里利用它来改变 output 的 channel, 具体说这里是减少 channel 数，从而达到减少计算的目的。
- 3 用 Global Ave Pool 取代 FC。下图可以看到，对于 FC ，超参数的个数为 7x7x1024x1024=51.3M，但是换成 Ave Pool之后，超参数变为0，所以这里可以起到防止过拟合的作用，另外作者发现采用 Ave Pool 之后，top-1的精度提高了大概0.6%。但是需要注意的是，在 GoogLeNet 中并没有完全取代 FC。在卷积和全连接之间采用了avepool。

![image](https://github.com/BlackApple-LMZ/paper_learning/edit/main/2021/0205-inceptionv1%20Going%20deeper%20with%20convolutions/22.png)

- 4 采用了辅助分类器。整个模型有三个 output（之前的网络都只有一个 output），这里的多个 output 仅仅在训练的时候用，也就是说测试或者部署的时候仅仅用最后一个输出。在训练的时候，将三个输出的loss进行加权平均，weight=0.3, 通过这种方式可以缓解梯度消失，同时作者也表示有正则化的作用。其实这个思想有点类似于传统机器学习中的投票机制，最终的结果由多个决策器共同投票决定，这个在传统机器学习中往往能提升大概2%的精度。
- 
https://blog.csdn.net/weixin_33729196/article/details/94013122 inception深度解析 很好

1\*1卷积提高非线性的解释：

将两个卷积串联，就能组合出更多的非线性特征。举个例子，假设第1个3x3卷积＋激活函数近似于f1(x)=ax2+bx+c，第二个1x1卷积＋激活函数近似于f2(x)=mx2+nx+q，那f1(x)和f2(f1(x))比哪个非线性更强，更能模拟非线性的特征？答案是显而易见的。NIN的结构和传统的神经网络中多层的结构有些类似，后者的多层是跨越了不同尺寸的感受野（通过层与层中间加pool层），从而在更高尺度上提取出特征；NIN结构是在同一个尺度上的多层（中间没有pool层），从而在相同的感受野范围能提取更强的非线性。

使用1x1卷积进行降维，降低了计算复杂度。有人会问，用1x1卷积降到96个特征后特征数不就减少了么，会影响最后训练的效果么？答案是否定的，只要最后输出的特征数不变（256组），中间的降维类似于压缩的效果，并不影响最终训练的结果。

多个尺寸上进行卷积再聚合，一方面是增加了不同的尺度信息进行融合，另一方面相同尺度的聚在一块，这样相关性较强的在一起可以加速收敛。
