大名鼎鼎的resnet！

https://blog.csdn.net/weixin_42546496/article/details/87810889

### 摘要：
提出了一种残差学习框架来减轻网络训练，这些网络比以前使用的网络更深，残差网络很容易优化，并可以显著增加深度来提高准确性；比VGG深8倍但仍具有较低的复杂度。并且效果很惊艳

### 引入残差网络的思路：
当网络退化时，浅层网络能够达到比深层网络更好的训练效果，这时如果我们把低层的特征传到高层，那么效果应该至少不比浅层的网络效果差，或者说如果一个VGG-100网络在第98层使用的是和VGG-16第14层一模一样的特征，那么VGG-100的效果应该会和VGG-16的效果相同。所以，我们可以在VGG-100的98层和14层之间添加一条直接映射（Identity Mapping）来达到此效果。

### 介绍：
网络加深的问题：加深之后是否好训练，可能会有梯度消失/爆炸的问题。然而，这个问题通过normalized initialization and intermediate normalization layers（输入输出归一化，使数据相对集中到固定的区域）在很大程度上已经解决。所以residual的模块主要是为了提高准确性而不是说解决梯度消失和爆炸的问题。

当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。意外的是，这种退化不是由过拟合引起的，并且在适当的深度模型上添加更多的层会导致更高的训练误差：

![image](https://github.com/BlackApple-LMZ/paper_learning/edit/main/2021/0128-Deep%20Residual%20Learning%20for%20Image%20Recognition/11.png)

在本文中，我们通过引入深度残差学习框架解决了退化问题：

Residual module就是引入了shortcut，快捷连接[2, 33, 48]是那些跳过一层或更多层的连接。在这篇论文中，快捷连接简单地执行恒等映射，就是直接引入到下面（E-Net就是搞了maxpool和conv吧），并将其输出添加到堆叠层的输出（图2）。恒等快捷连接既不增加额外的参数也不增加计算复杂度。整个网络仍然可以由带有反向传播的SGD进行端到端的训练。恒等映射足以解决退化问题，引入Ws，仅为了匹配维度（匹配channel这个通道数，有两种策略：（A）快捷连接仍然执行恒等映射，额外填充零输入以增加维度。此选项不会引入额外的参数；（B）方程（2）中的投影快捷连接用于匹配维度（由1×1卷积完成）。如果是匹配H和W这两个维度，A和B两个选项执行时步长为2。）

优点：
- 1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；
- 2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。（提高训练误差，降低准确率）

![image](https://github.com/BlackApple-LMZ/paper_learning/edit/main/2021/0128-Deep%20Residual%20Learning%20for%20Image%20Recognition/22.png)

F是指plain network，H是指整个residual模块，x就是shortcut。F可以是两层或者3层，如果是一层就没有效果了，就和一层线性函数一样。
### 网络结构：
![image](https://github.com/BlackApple-LMZ/paper_learning/edit/main/2021/0128-Deep%20Residual%20Learning%20for%20Image%20Recognition/33.png)

其中两层的block和三层的block分别为：

![image](https://github.com/BlackApple-LMZ/paper_learning/edit/main/2021/0128-Deep%20Residual%20Learning%20for%20Image%20Recognition/44.png)

### 训练细节：
ImageNet中我们的实现遵循[21，40]（40是VGG论文，就是里面的随机裁剪）的实践。调整图像大小，其较短的边在[256,480]之间进行随机采样，用于尺度增强[40]。224×224裁剪是从图像或其水平翻转中随机采样，并逐像素减去均值[21]。使用了[21]中的标准颜色增强。在每个卷积之后和激活之前，我们采用批量归一化（BN）[16]。我们按照[12]的方法初始化权重，从零开始训练所有的简单/残差网络。我们使用批大小为256的SGD方法。学习速度从0.1开始，当误差稳定时学习率除以10，并且模型训练高达60×104次迭代。我们使用的权重衰减为0.0001，动量为0.9。根据[16]的实践，我们不使用丢弃[13]。

在测试阶段，为了比较学习我们采用标准的10-crop测试[21]。对于最好的结果，我们采用如[40, 12]中的全卷积形式，并在多尺度上对分数进行平均（图像归一化，短边位于{224, 256, 384, 480, 640}中）。
