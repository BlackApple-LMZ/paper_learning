### 摘要：
由于每层的参数在变化，所以输入数据之后得到的输出的概率分布也会变化，导致学习的效果变差。这叫internal covariate shift，这篇文章提出了Batch Normalization的概念，就是每层的输入进行归一化。可以使用更大的学习率，初值可以更随意，也起到了一定的正则化的效果。

Normalization via Mini-Batch Statistics：

因为在前向进行归一化之后，在反向传播阶段同样需要进行求导操作，所以在整个数据集上进行归一化就很慢。。。但是基于某个或者部分样本进行规范化又会changes the representation ability of a network。所以这篇文章在minibatch内归一化，再用可以学习的 γ 和 β 来拟合minibatch的统计量与整个训练集统计量之间的关系。妙啊！（另一种解释：如果我们擅自改变中间的数据分布，势必会影响到之后的网络层的结果，换言之，可能改变了网络原有的特征表达能力。因此，我们需要一些额外的措施来弥补这里的损失。就是引入γ 和 β）每层对应一组γ 和 β。

BN依赖于同一batch的所有样本，论文讲了BN的反向传播推导，
