https://github.com/halajun/vdo_slam https://blog.csdn.net/ns2942826077/article/details/107596400

双目或者RGBD相机，利用基于图像的语义信息同时完成机器人定位以及静态和动态物体的构建，同时可以估计场景中刚体目标的速度。单目版本还需要额外的视差估计模块。但是最佳使用的应该是基于基于双目的室外场景。引言和相关工作部分列述了一些相关的研究进展，可以作为扩展资料阅读。
### 摘要：
传统的SLAM系统大多数在动态环境下运行时通过删除或单独跟踪移动对象来简化问题，这限制了系统在高动态和非结构化环境的应用。本文提出了一种鲁棒的目标感知动态SLAM系统，利用语义信息，在未知目标形状或运动模型的情况下，对场景中的刚体进行运动估计。该方法将环境中动态和静态结构集成到一个统一的估计框架中，从而精确的估计机器人位姿和时空地图。我们提供了一种从运动物体的姿态变化中估计速度的方法，为复杂环境下的导航提供了基础。我们在大量真实室内外数据集上进行了测试，显示一致和实质性的改进，并提供了源代码。
### 主要贡献：
1. 动态场景中估计机器人位姿、静态动态3D点、对象运动的统一框架。
2. 最先进的动态物体SE3位姿估计，并且提取速度。
3. 利用语义信息跟踪动态物体的鲁棒算法并且可以处理遮挡时的分割失败。

就是可以实现：动态物体的像素级分割、追踪场景中的刚性动态物体并估计速度、机器人定位、静态和动态物体建图、动态刚性物体的姿态变化。

- 实例级别的语义分割：采用mask r-cnn网络，由coco数据集训练，未经过微调。
- 稠密光流估计：采用PWC-Net网络，由FlyingChairs数据集训练，经过Sintel 和 KITTI 数据集微调。
- 立体匹配视差图：单目情况下，使用 MonoDepth2 (Godard et al. (2019)) 网络。数据集：Depth Eigen split

这篇文章具体的方法有很多东西，没有太仔细的看，以后用到这方面的工作再看看；
