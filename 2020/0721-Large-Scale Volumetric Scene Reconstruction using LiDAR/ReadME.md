大规模场景下，利用激光雷达传感器进行体积与深度融合的三维重建。用于自动驾驶或者机器人。
相关的工作：
目前有很多表示重建场景的方法：
1采用基于激光雷达的SLAM系统产生累积的点云从而描述场景，缺点是存储了很多冗余的数据降低后期的处理有效性，点云的密度随传感器的分辨率变化很大，并且远处的区域要比近处的稀疏很多。此外重建没有包含拓扑信息。
2用小的surface patch来表示。好处是不需要空间结构信息就能完成新的观测与旧的数据的融合，但是缺少真实的密度信息。
3离散空间成voxel grid。像占据栅格，用概率来进行融合，对于规划应用很好，对于其他应用比较粗糙。
4就是volumetric depth fusion。每个voxel包含到最近的surface的TSDF，这种方法以Kinect Fusion为代表，采用高性能显卡加kinect，具有很好的重建细节，同时能滤除传感器噪声。也可以通过对两个voxel的SDF进行插值实现表面重建。

贡献：
采用柱面投影模型，利用激光雷达数据进行体积深度融合。
1提出了建图框架，可以处理回环闭合，在记录过程中对多次访问的区域进行一致性重建。   
2分析了使用旋转式激光雷达传感器时必须考虑的所有相关影响，如非单视点和滚动快门。
3在公共数据集上进行验证。

总体框架：（包括CPU与GPU之间的数据传输）
